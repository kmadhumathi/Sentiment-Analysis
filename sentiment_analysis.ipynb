{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = pd.read_csv('all/train.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400001\n",
      "(50,)\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "word_vectors = []\n",
    "word_to_indx = {}\n",
    "indx = 0\n",
    "with open('glove.6B/glove.6B.50d.txt',encoding='utf-8') as file:\n",
    "    for indx,line in enumerate(file):\n",
    "        l = line.split()\n",
    "        word = l[0]\n",
    "        words.append(word)\n",
    "        word_to_indx[word] = indx+1\n",
    "        word_vec = np.asarray(l[1:],dtype='float32')\n",
    "        word_vectors.append(word_vec)\n",
    "\n",
    "word_to_indx['<PAD>'] = 0\n",
    "word_vectors.insert(0,np.zeros(word_vec.shape))\n",
    "print(len(word_vectors))\n",
    "print(word_vectors[0].shape)\n",
    "print(word_vec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_new = np.vstack(word_vectors)\n",
    "#word_vectors_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    if word in word_to_indx:\n",
    "        return word_vectors_new[word_to_indx[word]]\n",
    "    else:\n",
    "        return word_vectors_new[0]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def get_seq_embedding(sentence):\n",
    "    tokens = text_to_word_sequence(sentence)\n",
    "    embeddings = []\n",
    "    for i in tokens:\n",
    "        embeddings.append(get_word_embedding(i.lower()))\n",
    "    #for i in range(0,max_len-len(tokens)):\n",
    "    #    embeddings.append(get_word_embedding('<PAD>'))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_selected = train_file[['Phrase','Sentiment']]\n",
    "#x_train = [get_seq_embedding(i) for i in train]\n",
    "#y_train = np.asarray(train['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(train_file_selected)) < 0.8\n",
    "train = train_file_selected[msk]\n",
    "test = train_file_selected[~msk]\n",
    "train.reset_index(drop=True,inplace=True)\n",
    "test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124914, 31146, 156060)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test), len(train_file_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch train data generator\n",
    "'''\n",
    "temp = x[:10]\n",
    "tf.convert_to_tensor(temp)\n",
    "batched_data = tf.train.batch(tensors=x,batch_size=5,dynamic_pad=True)\n",
    "'''\n",
    "\n",
    "def generate_train_batch(batch_size):\n",
    "    sample_indices = np.random.choice(train.shape[0],size=batch_size,replace=False)\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in sample_indices:\n",
    "        x.append(get_seq_embedding(train['Phrase'][i]))\n",
    "        y.append(train['Sentiment'][i])\n",
    "    return pad_sequences(x,dtype='float32'),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch test data generator\n",
    "\n",
    "def generate_test_batch(batch_size):\n",
    "    sample_indices = np.random.choice(test.shape[0],size=batch_size,replace=False)\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in sample_indices:\n",
    "        x.append(get_seq_embedding(test['Phrase'][i]))\n",
    "        y.append(test['Sentiment'][i])\n",
    "    return pad_sequences(x,dtype='float32'),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nembedding_size = 100\\nlearning_rate = 0.0001\\n\\ngraph_input = tf.placeholder(shape=(None,None,embedding_size),dtype=tf.float32)\\ngraph_output = tf.placeholder(shape=(None),dtype=tf.int64)\\ngraph_training = tf.placeholder(shape=(), dtype=tf.bool)\\n\\n#rnn_cell = tf.keras.layers.SimpleRNNCell(units=128)\\nrnn_cell = tf.nn.rnn_cell.LSTMCell(128)\\nrnn_out,rnn_state = tf.nn.dynamic_rnn(cell=rnn_cell,inputs=graph_input,dtype=tf.float32)\\ndense = tf.layers.dense(inputs=rnn_out[:,-1,:],units=256,activation=tf.nn.relu)\\ndense = tf.layers.dense(inputs=dense,units=128,activation=tf.nn.relu)\\ndropout = =tf.layers.dropout\\ndense = tf.layers.dense(inputs=dense,units=64,activation=tf.nn.relu)\\nlogits = tf.layers.dense(inputs=dense,units=5,activation=tf.nn.relu)\\nprobs = tf.nn.softmax(logits)\\nlabels = tf.argmax(probs,axis=1)\\n#print(rnn_out.get_shape(),dense.get_shape())\\n#print(logits.get_shape(),probs.get_shape(),labels.get_shape())\\ncross_entropy_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=graph_output))\\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_loss)\\ncorrect_labels = tf.equal(labels,graph_output)\\naccuracy = tf.reduce_mean(tf.cast(correct_labels,tf.float32))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "'''\n",
    "embedding_size = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "graph_input = tf.placeholder(shape=(None,None,embedding_size),dtype=tf.float32)\n",
    "graph_output = tf.placeholder(shape=(None),dtype=tf.int64)\n",
    "graph_training = tf.placeholder(shape=(), dtype=tf.bool)\n",
    "\n",
    "#rnn_cell = tf.keras.layers.SimpleRNNCell(units=128)\n",
    "rnn_cell = tf.nn.rnn_cell.LSTMCell(128)\n",
    "rnn_out,rnn_state = tf.nn.dynamic_rnn(cell=rnn_cell,inputs=graph_input,dtype=tf.float32)\n",
    "dense = tf.layers.dense(inputs=rnn_out[:,-1,:],units=256,activation=tf.nn.relu)\n",
    "dense = tf.layers.dense(inputs=dense,units=128,activation=tf.nn.relu)\n",
    "dropout = =tf.layers.dropout\n",
    "dense = tf.layers.dense(inputs=dense,units=64,activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(inputs=dense,units=5,activation=tf.nn.relu)\n",
    "probs = tf.nn.softmax(logits)\n",
    "labels = tf.argmax(probs,axis=1)\n",
    "#print(rnn_out.get_shape(),dense.get_shape())\n",
    "#print(logits.get_shape(),probs.get_shape(),labels.get_shape())\n",
    "cross_entropy_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=graph_output))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
    "correct_labels = tf.equal(labels,graph_output)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_labels,tf.float32))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self,is_training,embedding_size,learning_rate,dropout_rate):\n",
    "        self.is_training = is_training\n",
    "        self.embedding_size = embedding_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build(self):\n",
    "        self.graph_input = tf.placeholder(shape=(None,None,self.embedding_size),dtype=tf.float32)\n",
    "        self.graph_output = tf.placeholder(shape=(None),dtype=tf.int64)\n",
    "        self.graph_training = tf.placeholder(shape=(), dtype=tf.bool)\n",
    "        \n",
    "        rnn_cell = tf.nn.rnn_cell.LSTMCell(10)\n",
    "        rnn_out,rnn_state = tf.nn.dynamic_rnn(cell=rnn_cell,inputs=self.graph_input,dtype=tf.float32)\n",
    "        dense = tf.layers.dense(inputs=rnn_out[:,-1,:],units=256,activation=tf.nn.relu)\n",
    "        dropout = tf.layers.dropout(inputs=dense,rate=self.dropout_rate,training=self.is_training)\n",
    "        dense = tf.layers.dense(inputs=dropout,units=1200,activation=tf.nn.relu)\n",
    "        self.logits = tf.layers.dense(inputs=dense,units=5,activation=tf.nn.relu)\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        self.labels = tf.argmax(self.probs,axis=1)\n",
    "        #print(rnn_out.get_shape(),dense.get_shape())\n",
    "        #print(logits.get_shape(),probs.get_shape(),labels.get_shape())\n",
    "        self.cross_entropy_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,labels=self.graph_output))\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.cross_entropy_loss)\n",
    "        self.correct_labels = tf.equal(self.labels,self.graph_output)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_labels,tf.float32))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModelPath = '/Users/m0k00ew/Desktop/MLDL/kaggle/models_saved/lstme60b32lr0001dout6emb50/bestModel.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: /Users/m0k00ew/Desktop/MLDL/kaggle/models_saved/lstme60b32lr0001dout6emb50/bestModel.ckpt\n",
      "epoch 0\n",
      "train loss 1.1796503\n",
      "train acc 0.546935\n",
      "test loss 1.1481545\n",
      "test acc 0.5648767\n",
      "\n",
      "\n",
      "epoch 1\n",
      "train loss 1.1414216\n",
      "train acc 0.5635969\n",
      "test loss 1.1411219\n",
      "test acc 0.56391317\n",
      "\n",
      "\n",
      "Model saved in path: /Users/m0k00ew/Desktop/MLDL/kaggle/models_saved/lstme60b32lr0001dout6emb50/bestModel.ckpt\n",
      "epoch 2\n",
      "train loss 1.1261456\n",
      "train acc 0.56916153\n",
      "test loss 1.1141078\n",
      "test acc 0.5754432\n",
      "\n",
      "\n",
      "epoch 3\n",
      "train loss 1.1297284\n",
      "train acc 0.5666394\n",
      "test loss 1.1215314\n",
      "test acc 0.5747045\n",
      "\n",
      "\n",
      "epoch 4\n",
      "train loss 1.1190493\n",
      "train acc 0.56964195\n",
      "test loss 1.1332738\n",
      "test acc 0.56696427\n",
      "\n",
      "\n",
      "epoch 5\n",
      "train loss 1.114189\n",
      "train acc 0.57266045\n",
      "test loss 1.1337478\n",
      "test acc 0.56561536\n",
      "\n",
      "\n",
      "epoch 6\n",
      "train loss 1.1155199\n",
      "train acc 0.5698501\n",
      "test loss 1.1204203\n",
      "test acc 0.57377315\n",
      "\n",
      "\n",
      "epoch 7\n",
      "train loss 1.1092359\n",
      "train acc 0.57412565\n",
      "test loss 1.120158\n",
      "test acc 0.57287383\n",
      "\n",
      "\n",
      "epoch 8\n",
      "train loss 1.1094661\n",
      "train acc 0.575719\n",
      "test loss 1.1199377\n",
      "test acc 0.56991905\n",
      "\n",
      "\n",
      "epoch 9\n",
      "train loss 1.1067538\n",
      "train acc 0.5727645\n",
      "test loss 1.1124711\n",
      "test acc 0.5745118\n",
      "\n",
      "\n",
      "Model saved in path: /Users/m0k00ew/Desktop/MLDL/kaggle/models_saved/lstme60b32lr0001dout6emb50/bestModel.ckpt\n",
      "epoch 10\n",
      "train loss 1.0996302\n",
      "train acc 0.57636756\n",
      "test loss 1.1114295\n",
      "test acc 0.5776272\n",
      "\n",
      "\n",
      "epoch 11\n",
      "train loss 1.1017431\n",
      "train acc 0.575711\n",
      "test loss 1.1130388\n",
      "test acc 0.57711333\n",
      "\n",
      "\n",
      "epoch 12\n",
      "train loss 1.1016341\n",
      "train acc 0.57612735\n",
      "test loss 1.117334\n",
      "test acc 0.57406217\n",
      "\n",
      "\n",
      "epoch 13\n",
      "train loss 1.1052401\n",
      "train acc 0.5735892\n",
      "test loss 1.1193953\n",
      "test acc 0.5699833\n",
      "\n",
      "\n",
      "epoch 14\n",
      "train loss 1.0992179\n",
      "train acc 0.57692\n",
      "test loss 1.1097989\n",
      "test acc 0.57550746\n",
      "\n",
      "\n",
      "epoch 15\n",
      "train loss 1.099061\n",
      "train acc 0.5773604\n",
      "test loss 1.1243294\n",
      "test acc 0.57271326\n",
      "\n",
      "\n",
      "epoch 16\n",
      "train loss 1.0982553\n",
      "train acc 0.57652766\n",
      "test loss 1.1185657\n",
      "test acc 0.5711395\n",
      "\n",
      "\n",
      "epoch 17\n",
      "train loss 1.0962864\n",
      "train acc 0.5773924\n",
      "test loss 1.111851\n",
      "test acc 0.57547534\n",
      "\n",
      "\n",
      "epoch 18\n",
      "train loss 1.1004745\n",
      "train acc 0.57330096\n",
      "test loss 1.1108392\n",
      "test acc 0.5745761\n",
      "\n",
      "\n",
      "epoch 19\n",
      "train loss 1.0928583\n",
      "train acc 0.57840925\n",
      "test loss 1.1091633\n",
      "test acc 0.5768564\n",
      "\n",
      "\n",
      "epoch 20\n",
      "train loss 1.0952917\n",
      "train acc 0.57750446\n",
      "test loss 1.1192727\n",
      "test acc 0.570465\n",
      "\n",
      "\n",
      "epoch 21\n",
      "train loss 1.0929774\n",
      "train acc 0.57774466\n",
      "test loss 1.1190398\n",
      "test acc 0.5733556\n",
      "\n",
      "\n",
      "epoch 22\n",
      "train loss 1.092572\n",
      "train acc 0.578097\n",
      "test loss 1.1179763\n",
      "test acc 0.568538\n",
      "\n",
      "\n",
      "epoch 23\n",
      "train loss 1.0926303\n",
      "train acc 0.5770881\n",
      "test loss 1.1220849\n",
      "test acc 0.570722\n",
      "\n",
      "\n",
      "epoch 24\n",
      "train loss 1.0888157\n",
      "train acc 0.5768079\n",
      "test loss 1.1181798\n",
      "test acc 0.5716855\n",
      "\n",
      "\n",
      "epoch 25\n",
      "train loss 1.0905125\n",
      "train acc 0.57639956\n",
      "test loss 1.1188732\n",
      "test acc 0.5688913\n",
      "\n",
      "\n",
      "epoch 26\n",
      "train loss 1.090754\n",
      "train acc 0.5773364\n",
      "test loss 1.1232835\n",
      "test acc 0.5655832\n",
      "\n",
      "\n",
      "epoch 27\n",
      "train loss 1.0931965\n",
      "train acc 0.578137\n",
      "test loss 1.1204873\n",
      "test acc 0.564973\n",
      "\n",
      "\n",
      "epoch 28\n",
      "train loss 1.0822549\n",
      "train acc 0.5819882\n",
      "test loss 1.1192149\n",
      "test acc 0.5683132\n",
      "\n",
      "\n",
      "epoch 29\n",
      "train loss 1.0886654\n",
      "train acc 0.57749647\n",
      "test loss 1.1135813\n",
      "test acc 0.56821686\n",
      "\n",
      "\n",
      "epoch 30\n",
      "train loss 1.0925193\n",
      "train acc 0.575727\n",
      "test loss 1.1191903\n",
      "test acc 0.567157\n",
      "\n",
      "\n",
      "epoch 31\n",
      "train loss 1.0918868\n",
      "train acc 0.575727\n",
      "test loss 1.1183298\n",
      "test acc 0.57011175\n",
      "\n",
      "\n",
      "epoch 32\n",
      "train loss 1.088101\n",
      "train acc 0.57740843\n",
      "test loss 1.1251053\n",
      "test acc 0.56657887\n",
      "\n",
      "\n",
      "epoch 33\n",
      "train loss 1.088775\n",
      "train acc 0.5775525\n",
      "test loss 1.1105144\n",
      "test acc 0.57480085\n",
      "\n",
      "\n",
      "epoch 34\n",
      "train loss 1.0883634\n",
      "train acc 0.57749647\n",
      "test loss 1.1132549\n",
      "test acc 0.57364464\n",
      "\n",
      "\n",
      "epoch 35\n",
      "train loss 1.0860889\n",
      "train acc 0.5792499\n",
      "test loss 1.1160243\n",
      "test acc 0.56857014\n",
      "\n",
      "\n",
      "epoch 36\n",
      "train loss 1.088829\n",
      "train acc 0.5756069\n",
      "test loss 1.1194454\n",
      "test acc 0.56179345\n",
      "\n",
      "\n",
      "epoch 37\n",
      "train loss 1.0880301\n",
      "train acc 0.57737637\n",
      "test loss 1.1192812\n",
      "test acc 0.57184607\n",
      "\n",
      "\n",
      "epoch 38\n",
      "train loss 1.0909662\n",
      "train acc 0.57670385\n",
      "test loss 1.1236763\n",
      "test acc 0.57492936\n",
      "\n",
      "\n",
      "epoch 39\n",
      "train loss 1.0887737\n",
      "train acc 0.5767518\n",
      "test loss 1.1200441\n",
      "test acc 0.56667525\n",
      "\n",
      "\n",
      "epoch 40\n",
      "train loss 1.0899811\n",
      "train acc 0.57434183\n",
      "test loss 1.1114318\n",
      "test acc 0.5754111\n",
      "\n",
      "\n",
      "epoch 41\n",
      "train loss 1.0859165\n",
      "train acc 0.5777927\n",
      "test loss 1.1276286\n",
      "test acc 0.5597379\n",
      "\n",
      "\n",
      "epoch 42\n",
      "train loss 1.0856497\n",
      "train acc 0.57817703\n",
      "test loss 1.1239103\n",
      "test acc 0.57081836\n",
      "\n",
      "\n",
      "epoch 43\n",
      "train loss 1.0862606\n",
      "train acc 0.5771282\n",
      "test loss 1.1181687\n",
      "test acc 0.5684738\n",
      "\n",
      "\n",
      "epoch 44\n",
      "train loss 1.0873872\n",
      "train acc 0.5768079\n",
      "test loss 1.1083066\n",
      "test acc 0.5696942\n",
      "\n",
      "\n",
      "epoch 45\n",
      "train loss 1.0859917\n",
      "train acc 0.5771202\n",
      "test loss 1.1013683\n",
      "test acc 0.5742549\n",
      "\n",
      "\n",
      "epoch 46\n",
      "train loss 1.0869231\n",
      "train acc 0.57575905\n",
      "test loss 1.112096\n",
      "test acc 0.570465\n",
      "\n",
      "\n",
      "epoch 47\n",
      "train loss 1.0852603\n",
      "train acc 0.57772064\n",
      "test loss 1.1185665\n",
      "test acc 0.5644913\n",
      "\n",
      "\n",
      "epoch 48\n",
      "train loss 1.0891564\n",
      "train acc 0.5739415\n",
      "test loss 1.1156062\n",
      "test acc 0.566065\n",
      "\n",
      "\n",
      "epoch 49\n",
      "train loss 1.0870588\n",
      "train acc 0.5770641\n",
      "test loss 1.1255041\n",
      "test acc 0.563046\n",
      "\n",
      "\n",
      "epoch 50\n",
      "train loss 1.0900056\n",
      "train acc 0.5741016\n",
      "test loss 1.1235216\n",
      "test acc 0.5597058\n",
      "\n",
      "\n",
      "epoch 51\n",
      "train loss 1.0901966\n",
      "train acc 0.5724923\n",
      "test loss 1.1102691\n",
      "test acc 0.5680884\n",
      "\n",
      "\n",
      "epoch 52\n",
      "train loss 1.0849873\n",
      "train acc 0.5761514\n",
      "test loss 1.1160082\n",
      "test acc 0.5672212\n",
      "\n",
      "\n",
      "epoch 53\n",
      "train loss 1.0853143\n",
      "train acc 0.5788336\n",
      "test loss 1.1179873\n",
      "test acc 0.5628854\n",
      "\n",
      "\n",
      "epoch 54\n",
      "train loss 1.0842501\n",
      "train acc 0.57683194\n",
      "test loss 1.1210849\n",
      "test acc 0.557843\n",
      "\n",
      "\n",
      "epoch 55\n",
      "train loss 1.0807148\n",
      "train acc 0.5775525\n",
      "test loss 1.1218687\n",
      "test acc 0.56551903\n",
      "\n",
      "\n",
      "epoch 56\n",
      "train loss 1.0841879\n",
      "train acc 0.5763355\n",
      "test loss 1.1201415\n",
      "test acc 0.56648254\n",
      "\n",
      "\n",
      "epoch 57\n",
      "train loss 1.0876105\n",
      "train acc 0.57519054\n",
      "test loss 1.1146035\n",
      "test acc 0.57271326\n",
      "\n",
      "\n",
      "epoch 58\n",
      "train loss 1.0842733\n",
      "train acc 0.57527065\n",
      "test loss 1.1115236\n",
      "test acc 0.5664183\n",
      "\n",
      "\n",
      "epoch 59\n",
      "train loss 1.0851333\n",
      "train acc 0.57671183\n",
      "test loss 1.1120603\n",
      "test acc 0.5682811\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "model = Model(is_training=True,embedding_size=50,learning_rate=0.001,dropout_rate=0.6)\n",
    "model.build()\n",
    "\n",
    "epochs = 60 #100\n",
    "batch_size = 32\n",
    "best_test_acc = None\n",
    "sess = tf.Session()\n",
    "init = tf.initializers.global_variables()\n",
    "sess.run(init)\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    train_steps_per_epoch = train.shape[0]//batch_size\n",
    "    test_steps_per_epoch = test.shape[0]//batch_size\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    for train_step in range(0,train_steps_per_epoch):\n",
    "        x,y = generate_train_batch(batch_size)\n",
    "        #print(x.shape,len(y))\n",
    "        #rnn_o,dense_o,logits_o,probs_o,labels_o = sess.run([rnn_out,dense,logits,probs,labels],feed_dict={graph_input:x,graph_output:y})\n",
    "        #print(rnn_o.shape,logits_o.shape,probs_o.shape,labels_o.shape)\n",
    "        #_ = sess.run(optimizer,feed_dict={graph_input:x,graph_output:y})\n",
    "        cross_ent,_,correct_lab,acc = sess.run([model.cross_entropy_loss,model.optimizer,model.correct_labels,model.accuracy],feed_dict={model.graph_input:x,model.graph_output:y})\n",
    "        train_losses.append(cross_ent)\n",
    "        train_accs.append(acc)\n",
    "    \n",
    "    for test_step in range(0,test_steps_per_epoch):\n",
    "        x,y = generate_test_batch(batch_size)\n",
    "        cross_ent_test,correct_lab_test,acc_test = sess.run([model.cross_entropy_loss,model.correct_labels,model.accuracy],feed_dict={model.graph_input:x,model.graph_output:y})\n",
    "        test_losses.append(cross_ent_test)\n",
    "        test_accs.append(acc_test)\n",
    "    if best_test_acc is None or np.mean(test_accs) >= best_test_acc:\n",
    "        best_test_acc = np.mean(test_accs)\n",
    "        save_path = saver.save(sess, bestModelPath)\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "    print (\"epoch\",epoch)\n",
    "    print (\"train loss\",np.mean(train_losses))\n",
    "    print (\"train acc\",np.mean(train_accs))\n",
    "    print (\"test loss\",np.mean(test_losses))\n",
    "    print (\"test acc\",np.mean(test_accs))\n",
    "    print (\"\\n\")\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = pd.read_csv('all/test.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_file['Phrase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>156066</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>156067</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>156068</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>156069</td>\n",
       "      <td>8545</td>\n",
       "      <td>pleasing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>156070</td>\n",
       "      <td>8545</td>\n",
       "      <td>but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>156071</td>\n",
       "      <td>8545</td>\n",
       "      <td>mostly routine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>156072</td>\n",
       "      <td>8545</td>\n",
       "      <td>mostly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>156073</td>\n",
       "      <td>8545</td>\n",
       "      <td>routine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>156074</td>\n",
       "      <td>8545</td>\n",
       "      <td>effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>156075</td>\n",
       "      <td>8545</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>156076</td>\n",
       "      <td>8546</td>\n",
       "      <td>Kidman is really the only thing that 's worth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>156077</td>\n",
       "      <td>8546</td>\n",
       "      <td>Kidman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>156078</td>\n",
       "      <td>8546</td>\n",
       "      <td>is really the only thing that 's worth watchin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>156079</td>\n",
       "      <td>8546</td>\n",
       "      <td>is really the only thing that 's worth watchin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>156080</td>\n",
       "      <td>8546</td>\n",
       "      <td>is really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>156081</td>\n",
       "      <td>8546</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>156082</td>\n",
       "      <td>8546</td>\n",
       "      <td>really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>156083</td>\n",
       "      <td>8546</td>\n",
       "      <td>the only thing that 's worth watching in Birth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>156084</td>\n",
       "      <td>8546</td>\n",
       "      <td>the only thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>156085</td>\n",
       "      <td>8546</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>156086</td>\n",
       "      <td>8546</td>\n",
       "      <td>only thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>156087</td>\n",
       "      <td>8546</td>\n",
       "      <td>only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>156088</td>\n",
       "      <td>8546</td>\n",
       "      <td>thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>156089</td>\n",
       "      <td>8546</td>\n",
       "      <td>that 's worth watching in Birthday Girl , a fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>156090</td>\n",
       "      <td>8546</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66262</th>\n",
       "      <td>222323</td>\n",
       "      <td>11853</td>\n",
       "      <td>organized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66263</th>\n",
       "      <td>222324</td>\n",
       "      <td>11853</td>\n",
       "      <td>efficiency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66264</th>\n",
       "      <td>222325</td>\n",
       "      <td>11853</td>\n",
       "      <td>numerous flashbacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66265</th>\n",
       "      <td>222326</td>\n",
       "      <td>11853</td>\n",
       "      <td>a constant edge of tension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66266</th>\n",
       "      <td>222327</td>\n",
       "      <td>11853</td>\n",
       "      <td>a constant edge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66267</th>\n",
       "      <td>222328</td>\n",
       "      <td>11853</td>\n",
       "      <td>constant edge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66268</th>\n",
       "      <td>222329</td>\n",
       "      <td>11853</td>\n",
       "      <td>, Miller 's film is one of 2002 's involvingly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66269</th>\n",
       "      <td>222330</td>\n",
       "      <td>11853</td>\n",
       "      <td>Miller 's film is one of 2002 's involvingly a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66270</th>\n",
       "      <td>222331</td>\n",
       "      <td>11853</td>\n",
       "      <td>Miller 's film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66271</th>\n",
       "      <td>222332</td>\n",
       "      <td>11853</td>\n",
       "      <td>is one of 2002 's involvingly adult surprises .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66272</th>\n",
       "      <td>222333</td>\n",
       "      <td>11853</td>\n",
       "      <td>is one of 2002 's involvingly adult surprises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66273</th>\n",
       "      <td>222334</td>\n",
       "      <td>11853</td>\n",
       "      <td>one of 2002 's involvingly adult surprises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66274</th>\n",
       "      <td>222335</td>\n",
       "      <td>11853</td>\n",
       "      <td>of 2002 's involvingly adult surprises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66275</th>\n",
       "      <td>222336</td>\n",
       "      <td>11853</td>\n",
       "      <td>2002 's involvingly adult surprises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66276</th>\n",
       "      <td>222337</td>\n",
       "      <td>11853</td>\n",
       "      <td>2002 's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66277</th>\n",
       "      <td>222338</td>\n",
       "      <td>11853</td>\n",
       "      <td>involvingly adult surprises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66278</th>\n",
       "      <td>222339</td>\n",
       "      <td>11853</td>\n",
       "      <td>involvingly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66279</th>\n",
       "      <td>222340</td>\n",
       "      <td>11853</td>\n",
       "      <td>adult surprises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66280</th>\n",
       "      <td>222341</td>\n",
       "      <td>11854</td>\n",
       "      <td>They should have called it Gutterball .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66281</th>\n",
       "      <td>222342</td>\n",
       "      <td>11854</td>\n",
       "      <td>should have called it Gutterball .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66282</th>\n",
       "      <td>222343</td>\n",
       "      <td>11854</td>\n",
       "      <td>should have called it Gutterball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66283</th>\n",
       "      <td>222344</td>\n",
       "      <td>11854</td>\n",
       "      <td>have called it Gutterball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66284</th>\n",
       "      <td>222345</td>\n",
       "      <td>11854</td>\n",
       "      <td>called it Gutterball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66285</th>\n",
       "      <td>222346</td>\n",
       "      <td>11854</td>\n",
       "      <td>it Gutterball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66286</th>\n",
       "      <td>222347</td>\n",
       "      <td>11854</td>\n",
       "      <td>Gutterball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66287</th>\n",
       "      <td>222348</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded , predictable scenario .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66288</th>\n",
       "      <td>222349</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded , predictable scenario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66289</th>\n",
       "      <td>222350</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66290</th>\n",
       "      <td>222351</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66291</th>\n",
       "      <td>222352</td>\n",
       "      <td>11855</td>\n",
       "      <td>predictable scenario</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66292 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PhraseId  SentenceId                                             Phrase\n",
       "0        156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1        156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2        156063        8545                                                 An\n",
       "3        156064        8545  intermittently pleasing but mostly routine effort\n",
       "4        156065        8545         intermittently pleasing but mostly routine\n",
       "5        156066        8545                        intermittently pleasing but\n",
       "6        156067        8545                            intermittently pleasing\n",
       "7        156068        8545                                     intermittently\n",
       "8        156069        8545                                           pleasing\n",
       "9        156070        8545                                                but\n",
       "10       156071        8545                                     mostly routine\n",
       "11       156072        8545                                             mostly\n",
       "12       156073        8545                                            routine\n",
       "13       156074        8545                                             effort\n",
       "14       156075        8545                                                  .\n",
       "15       156076        8546  Kidman is really the only thing that 's worth ...\n",
       "16       156077        8546                                             Kidman\n",
       "17       156078        8546  is really the only thing that 's worth watchin...\n",
       "18       156079        8546  is really the only thing that 's worth watchin...\n",
       "19       156080        8546                                          is really\n",
       "20       156081        8546                                                 is\n",
       "21       156082        8546                                             really\n",
       "22       156083        8546  the only thing that 's worth watching in Birth...\n",
       "23       156084        8546                                     the only thing\n",
       "24       156085        8546                                                the\n",
       "25       156086        8546                                         only thing\n",
       "26       156087        8546                                               only\n",
       "27       156088        8546                                              thing\n",
       "28       156089        8546  that 's worth watching in Birthday Girl , a fi...\n",
       "29       156090        8546                                               that\n",
       "...         ...         ...                                                ...\n",
       "66262    222323       11853                                          organized\n",
       "66263    222324       11853                                         efficiency\n",
       "66264    222325       11853                                numerous flashbacks\n",
       "66265    222326       11853                         a constant edge of tension\n",
       "66266    222327       11853                                    a constant edge\n",
       "66267    222328       11853                                      constant edge\n",
       "66268    222329       11853  , Miller 's film is one of 2002 's involvingly...\n",
       "66269    222330       11853  Miller 's film is one of 2002 's involvingly a...\n",
       "66270    222331       11853                                     Miller 's film\n",
       "66271    222332       11853    is one of 2002 's involvingly adult surprises .\n",
       "66272    222333       11853      is one of 2002 's involvingly adult surprises\n",
       "66273    222334       11853         one of 2002 's involvingly adult surprises\n",
       "66274    222335       11853             of 2002 's involvingly adult surprises\n",
       "66275    222336       11853                2002 's involvingly adult surprises\n",
       "66276    222337       11853                                            2002 's\n",
       "66277    222338       11853                        involvingly adult surprises\n",
       "66278    222339       11853                                        involvingly\n",
       "66279    222340       11853                                    adult surprises\n",
       "66280    222341       11854            They should have called it Gutterball .\n",
       "66281    222342       11854                 should have called it Gutterball .\n",
       "66282    222343       11854                   should have called it Gutterball\n",
       "66283    222344       11854                          have called it Gutterball\n",
       "66284    222345       11854                               called it Gutterball\n",
       "66285    222346       11854                                      it Gutterball\n",
       "66286    222347       11854                                         Gutterball\n",
       "66287    222348       11855             A long-winded , predictable scenario .\n",
       "66288    222349       11855               A long-winded , predictable scenario\n",
       "66289    222350       11855                                    A long-winded ,\n",
       "66290    222351       11855                                      A long-winded\n",
       "66291    222352       11855                               predictable scenario\n",
       "\n",
       "[66292 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = [get_seq_embedding(i) for i in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pad_sequences(x_test,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/m0k00ew/Desktop/MLDL/kaggle/models_saved/lstme60b32lr0001dout6emb50/bestModel.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = Model(is_training=False,embedding_size=50,learning_rate=0,dropout_rate=0)\n",
    "model.build()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "\n",
    "saver.restore(sess, bestModelPath)\n",
    "\n",
    "init = tf.initializers.global_variables()\n",
    "sess.run(init)\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "test_probs,test_labels = sess.run([model.probs,model.labels],feed_dict={model.graph_input:x_test})\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(data={'PhraseId':test_file['PhraseId'],'Sentiment':test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submit_lstm_epoch60_batch32_0001_dout6_emb50.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
